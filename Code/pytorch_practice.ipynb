{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Datasets Building\n",
    "class DiabetesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.data_original = np.loadtxt(filepath, skiprows=1, delimiter=',')\n",
    "        self.len = self.data_original[0]\n",
    "        self.x_data = torch.Tensor(torch.from_numpy(self.data_original[:, :-1]))\n",
    "        self.y_data = torch.Tensor(torch.from_numpy(self.data_original[:, [-1]]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "file = 'pima_indians_diabetes.csv'\n",
    "mydataset = DiabetesDataset(file)\n",
    "trainloader = DataLoader(dataset=(mydataset.x_data,mydataset.y_data), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, dim1, bias=False)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(dim1, dim2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(dim2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        output = self.activation1(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.activation2(output)\n",
    "        output = self.linear3(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "model = MyModel(64, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [768, 1] at entry 0 and [768, 8] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mfor\u001b[39;00m a, (x,y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[0;32m      3\u001b[0m         pred \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m      4\u001b[0m         \u001b[39m# print(pred)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wxx19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\wxx19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\wxx19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\wxx19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\wxx19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\wxx19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [768, 1] at entry 0 and [768, 8] at entry 1"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for a, (x,y) in enumerate(trainloader):\n",
    "        pred = model(x)\n",
    "        # print(pred)\n",
    "        loss = loss_fn(pred, y)\n",
    "        if a%10 == 0:\n",
    "            print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Datasets\n",
    "df_tensor = torch.Tensor(df.values)\n",
    "x_tensor = torch.Tensor(x.values)\n",
    "y_tensor = torch.Tensor(y.values)\n",
    "x_train_tensor = torch.Tensor(x_train.values)\n",
    "y_train_tensor = torch.Tensor(y_train.values)\n",
    "x_test_tensor = torch.Tensor(x_test.values)\n",
    "y_test_tensor = torch.Tensor(y_test.values)\n",
    "# df_tensor = torch.Tensor(torch.from_numpy(df.to_numpy()))\n",
    "# x_tensor = torch.DoubleTensor(torch.from_numpy(x.to_numpy()))\n",
    "# y_tensor = torch.LongTensor(torch.from_numpy(y.to_numpy()))\n",
    "# x_train_tensor = torch.DoubleTensor(torch.from_numpy(x_train.to_numpy()))\n",
    "# y_train_tensor = torch.LongTensor(torch.from_numpy(y_train.to_numpy()))\n",
    "# x_test_tensor = torch.DoubleTensor(torch.from_numpy(x_test.to_numpy()))\n",
    "# y_test_tensor = torch.LongTensor(torch.from_numpy(y_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(TensorDataset(x_train_tensor, y_train_tensor), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, dim1, bias=False)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(dim1, dim2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(dim2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        output = self.activation1(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.activation2(output)\n",
    "        output = self.linear3(output)\n",
    "        return output\n",
    "    \n",
    "model = MyModel(64, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for a, (x,y) in enumerate(trainloader):\n",
    "    pred = model(x)\n",
    "    # print(pred)\n",
    "    loss = loss_fn(pred, y)\n",
    "    if a%10 == 0:\n",
    "        print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sigmoid.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[138], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pred \u001b[39m=\u001b[39m model(x_test_tensor)\n\u001b[1;32m----> 3\u001b[0m sigmoid \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mSigmoid(pred)\n\u001b[0;32m      4\u001b[0m pred \u001b[39m=\u001b[39m sigmoid(pred)\n\u001b[0;32m      5\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax\n",
      "File \u001b[1;32mc:\\Users\\wxx19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:449\u001b[0m, in \u001b[0;36mModule.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))))\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mlen\u001b[39m(args) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m    452\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[39msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Sigmoid.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred = model(x_test_tensor)\n",
    "sigmoid = nn.Sigmoid(pred)\n",
    "pred = sigmoid(pred)\n",
    "result = np.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2824],\n",
       "        [-0.3201],\n",
       "        [-0.3150],\n",
       "        [-0.3372],\n",
       "        [-0.3173],\n",
       "        [-0.2499],\n",
       "        [-0.3576],\n",
       "        [-0.3104],\n",
       "        [-0.2772],\n",
       "        [-0.2491],\n",
       "        [-0.3099],\n",
       "        [-0.2342],\n",
       "        [-0.2888],\n",
       "        [-0.2978],\n",
       "        [-0.3267],\n",
       "        [-0.2924],\n",
       "        [-0.3020],\n",
       "        [-0.3297],\n",
       "        [-0.2868],\n",
       "        [-0.2922],\n",
       "        [-0.2916],\n",
       "        [-0.3344],\n",
       "        [-0.2162],\n",
       "        [-0.3118],\n",
       "        [-0.3196],\n",
       "        [-0.2784],\n",
       "        [-0.3070],\n",
       "        [-0.3596],\n",
       "        [-0.2728],\n",
       "        [-0.3313],\n",
       "        [-0.2837],\n",
       "        [-0.2274],\n",
       "        [-0.2405],\n",
       "        [-0.3204],\n",
       "        [-0.2642],\n",
       "        [-0.2759],\n",
       "        [-0.1793],\n",
       "        [-0.3172],\n",
       "        [-0.2917],\n",
       "        [-0.2969],\n",
       "        [-0.3277],\n",
       "        [-0.2445],\n",
       "        [-0.2282],\n",
       "        [-0.3312],\n",
       "        [-0.3571],\n",
       "        [-0.2588],\n",
       "        [-0.3133],\n",
       "        [-0.2841],\n",
       "        [-0.2706],\n",
       "        [-0.3021],\n",
       "        [-0.3428],\n",
       "        [-0.3001],\n",
       "        [-0.2845],\n",
       "        [-0.3280],\n",
       "        [-0.3369],\n",
       "        [-0.3405],\n",
       "        [-0.2364],\n",
       "        [-0.3250],\n",
       "        [-0.3082],\n",
       "        [-0.2842],\n",
       "        [-0.2795],\n",
       "        [-0.3080],\n",
       "        [-0.3433],\n",
       "        [-0.3332],\n",
       "        [-0.3083],\n",
       "        [-0.2601],\n",
       "        [-0.3379],\n",
       "        [-0.2699],\n",
       "        [-0.3536],\n",
       "        [-0.2107],\n",
       "        [-0.2717],\n",
       "        [-0.3468],\n",
       "        [-0.2701],\n",
       "        [-0.3290],\n",
       "        [-0.3382],\n",
       "        [-0.2158],\n",
       "        [-0.2918],\n",
       "        [-0.3311],\n",
       "        [-0.2857],\n",
       "        [-0.3296],\n",
       "        [-0.2944],\n",
       "        [-0.3340],\n",
       "        [-0.3326],\n",
       "        [-0.2656],\n",
       "        [-0.3121],\n",
       "        [-0.2981],\n",
       "        [-0.2481],\n",
       "        [-0.2919],\n",
       "        [-0.3204],\n",
       "        [-0.3053],\n",
       "        [-0.3262],\n",
       "        [-0.3000],\n",
       "        [-0.3267],\n",
       "        [-0.3085],\n",
       "        [-0.2919],\n",
       "        [-0.2786],\n",
       "        [-0.2818],\n",
       "        [-0.3082],\n",
       "        [-0.2185],\n",
       "        [-0.3288],\n",
       "        [-0.2203],\n",
       "        [-0.3235],\n",
       "        [-0.3051],\n",
       "        [-0.2628],\n",
       "        [-0.3193],\n",
       "        [-0.2730],\n",
       "        [-0.3074],\n",
       "        [-0.2785],\n",
       "        [-0.3458],\n",
       "        [-0.2946],\n",
       "        [-0.3243],\n",
       "        [-0.3359],\n",
       "        [-0.3287],\n",
       "        [-0.2298],\n",
       "        [-0.2930],\n",
       "        [-0.3113],\n",
       "        [-0.3030],\n",
       "        [-0.3063],\n",
       "        [-0.3312],\n",
       "        [-0.3252],\n",
       "        [-0.3412],\n",
       "        [-0.3112],\n",
       "        [-0.3178],\n",
       "        [-0.3293],\n",
       "        [-0.3286],\n",
       "        [-0.1981],\n",
       "        [-0.3280],\n",
       "        [-0.3236],\n",
       "        [-0.2941],\n",
       "        [-0.3080],\n",
       "        [-0.3004],\n",
       "        [-0.3049],\n",
       "        [-0.3235],\n",
       "        [-0.3000],\n",
       "        [-0.2391],\n",
       "        [-0.3361],\n",
       "        [-0.3275],\n",
       "        [-0.2990],\n",
       "        [-0.2790],\n",
       "        [-0.3667],\n",
       "        [-0.3455],\n",
       "        [-0.3540],\n",
       "        [-0.3397],\n",
       "        [-0.2233],\n",
       "        [-0.3004],\n",
       "        [-0.2623],\n",
       "        [-0.3386],\n",
       "        [-0.3329],\n",
       "        [-0.2450],\n",
       "        [-0.2986],\n",
       "        [-0.3347],\n",
       "        [-0.2444],\n",
       "        [-0.2649],\n",
       "        [-0.2515],\n",
       "        [-0.3237],\n",
       "        [-0.3203],\n",
       "        [-0.3194],\n",
       "        [-0.2025],\n",
       "        [-0.2756],\n",
       "        [-0.3274],\n",
       "        [-0.3174],\n",
       "        [-0.3069],\n",
       "        [-0.2965],\n",
       "        [-0.3717],\n",
       "        [-0.3186],\n",
       "        [-0.3041],\n",
       "        [-0.3319],\n",
       "        [-0.3351],\n",
       "        [-0.3186],\n",
       "        [-0.3162],\n",
       "        [-0.2870],\n",
       "        [-0.3245],\n",
       "        [-0.2886],\n",
       "        [-0.2852],\n",
       "        [-0.2603],\n",
       "        [-0.3324],\n",
       "        [-0.3093],\n",
       "        [-0.2981],\n",
       "        [-0.3283],\n",
       "        [-0.3327],\n",
       "        [-0.3363],\n",
       "        [-0.2643],\n",
       "        [-0.3000],\n",
       "        [-0.2582],\n",
       "        [-0.2826],\n",
       "        [-0.3309],\n",
       "        [-0.3175],\n",
       "        [-0.2978],\n",
       "        [-0.2670],\n",
       "        [-0.2753],\n",
       "        [-0.3133],\n",
       "        [-0.3458],\n",
       "        [-0.2927],\n",
       "        [-0.2990],\n",
       "        [-0.3027],\n",
       "        [-0.3200],\n",
       "        [-0.3304],\n",
       "        [-0.3310],\n",
       "        [-0.3111],\n",
       "        [-0.2522],\n",
       "        [-0.3454],\n",
       "        [-0.3256],\n",
       "        [-0.2582],\n",
       "        [-0.3045],\n",
       "        [-0.2993],\n",
       "        [-0.3243],\n",
       "        [-0.3010],\n",
       "        [-0.2931],\n",
       "        [-0.2549],\n",
       "        [-0.3082],\n",
       "        [-0.2865],\n",
       "        [-0.3129],\n",
       "        [-0.3158],\n",
       "        [-0.3336],\n",
       "        [-0.2802],\n",
       "        [-0.3296],\n",
       "        [-0.2560],\n",
       "        [-0.3178],\n",
       "        [-0.3389],\n",
       "        [-0.3207],\n",
       "        [-0.2741],\n",
       "        [-0.3151],\n",
       "        [-0.3283],\n",
       "        [-0.3349],\n",
       "        [-0.3408],\n",
       "        [-0.2947],\n",
       "        [-0.3300],\n",
       "        [-0.3124],\n",
       "        [-0.2840],\n",
       "        [-0.2747],\n",
       "        [-0.2490]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = torch.LongTensor(y_train.values)\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_tensor[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download dataset from github\n",
    "def download_file(url):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "file_url = 'https://raw.githubusercontent.com/xinxiewu/datasets/main/pima_indians_diabetes.csv'\n",
    "df = pd.read_csv(download_file(file_url))\n",
    "\n",
    "# Diabetes Group\n",
    "df.loc[(df['diabetes'] == 1) & (df['glucose'] == 0), 'glucose'] = 140\n",
    "df.loc[(df['diabetes'] == 1) & (df['bp'] == 0), 'bp'] = 74\n",
    "df.loc[(df['diabetes'] == 1) & (df['skin_thick'] == 0), 'skin_thick'] = 27\n",
    "df.loc[(df['diabetes'] == 1) & (df['insulin'] == 0), 'insulin'] = 100\n",
    "df.loc[(df['diabetes'] == 1) & (df['bmi'] == 0), 'bmi'] = 34.25\n",
    "df.loc[(df['diabetes'] == 1) & (df['pedigree'] == 0), 'pedigree'] = 0.449\n",
    "# Non-Diabetes Group\n",
    "df.loc[(df['diabetes'] == 0) & (df['glucose'] == 0), 'glucose'] = 107\n",
    "df.loc[(df['diabetes'] == 0) & (df['bp'] == 0), 'bp'] = 70\n",
    "df.loc[(df['diabetes'] == 0) & (df['skin_thick'] == 0), 'skin_thick'] = 21\n",
    "df.loc[(df['diabetes'] == 0) & (df['insulin'] == 0), 'insulin'] = 68.792\n",
    "df.loc[(df['diabetes'] == 0) & (df['bmi'] == 0), 'bmi'] = 30.05\n",
    "df.loc[(df['diabetes'] == 0) & (df['pedigree'] == 0), 'pedigree'] = 0.336\n",
    "\n",
    "# Normalization\n",
    "df.preg = (df.preg - df.preg.mean())/df.preg.std()\n",
    "df.glucose = (df.glucose - df.glucose.mean())/df.glucose.std()\n",
    "df.bp = (df.bp - df.bp.mean())/df.bp.std()\n",
    "df.skin_thick = (df.skin_thick - df.skin_thick.mean())/df.skin_thick.std()\n",
    "df.insulin = (df.insulin - df.insulin.mean())/df.insulin.std()\n",
    "df.bmi = (df.bmi - df.bmi.mean())/df.bmi.std()\n",
    "df.pedigree = (df.pedigree - df.pedigree.mean())/df.pedigree.std()\n",
    "df.age = (df.age - df.age.mean())/df.age.std()\n",
    "\n",
    "# 7:3 Data Split\n",
    "x = df.iloc[:, 0:8]\n",
    "y = df.iloc[:, 8:9]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
